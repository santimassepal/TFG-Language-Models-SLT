# TFG-Language-Models-SLT

This study is centred in evaluating the performance of different techniques for Sign Language Translation (SLT), with the intention of identifying potential lines of research for the following years. During the course of the project, an exhaustive state of the art review on Machine Learning, Natural Language Processing and Sign Language Translation is carried out, setting the foundations for the experimentation and obtained results interpretation.

The three techniques evaluated in this project are based on the innovative Transformers architecture and include the use of an Encoder-Decoder network from scratch trained exclusively with a Seq-to-seq dataset for SLT; The application of the BART model, fine-tuned specifically for this matter and the utilisation of GPT-3.5 and GPT-4 models without previous training due to the inability of performing fine-tuning on the fly.

This study reveals that the use of pre-trained models such as BART, GPT-3.5 and GPT-4 outperforms the other intended alternative due to the lack of available data and its nature in the SLT field. Thus, future lines of research could take into account the balance between the resource consumption and performance of these large language models, as well as the resource consumption and the carbon footprint of the refrigeration system of the servers that host these large language models.
