{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec8deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = 'facebook/bart-base'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062bee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['membership of parliament see minutes', 'approval of minutes of previous sitting see minutes', 'membership of parliament see minutes', 'verification of credentials see minutes', 'documents received see minutes', 'written statements and oral questions tabling see minutes', 'petitions see minutes', 'texts of agreements forwarded by the council see minutes', \"action taken on parliament's resolutions see minutes\", 'agenda for next sitting see minutes']\n",
      "['MEMBERSHIP PARLIAMENT SEE MINUTE', 'APPROVAL MINUTE DESC-PREVIOUS SIT SEE MINUTE', 'MEMBERSHIP PARLIAMENT SEE MINUTE', 'VERIFICATION CREDENTIALS SEE MINUTE', 'DOCUMENT RECEIVE SEE MINUTE', 'WRITE STATEMENT AND DESC-ORAL QUESTION TABLE SEE MINUTE', 'PETITION SEE MINUTE', 'TEXT AGREEMENT DESC-FORWARD BY COUNCIL SEE MINUTE', 'ACTION TAKE ON PARLIAMENT X-POSS RESOLUTION SEE MINUTE', 'AGENDA FOR NEXT SIT SEE MINUTE']\n"
     ]
    }
   ],
   "source": [
    "with open('eng-slg.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "source_texts, target_texts = [], []\n",
    "for line in lines:\n",
    "    source, target = line.split('@')\n",
    "    source_texts.append(source.strip())\n",
    "    target_texts.append(target.strip())\n",
    "    \n",
    "print(source_texts[0:10])\n",
    "print(target_texts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c3b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = tokenizer(source_texts, truncation=True, padding=True)\n",
    "target_encodings = tokenizer(target_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62be0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_encodings, target_encodings):\n",
    "        self.input_encodings = input_encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.input_encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.target_encodings['input_ids'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_encodings.input_ids)\n",
    "\n",
    "dataset = TranslationDataset(input_encodings, target_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idener\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset,               # training dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./my_bart_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
